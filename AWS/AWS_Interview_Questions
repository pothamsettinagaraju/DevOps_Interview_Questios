1.What is VPC and How it using in DevOps?
  A VPC is a Virtual Private Cloud by using which we would able to isolate our resources over a public cloud environment
  and by using this VPC I would be able to configure the CADR range for our particular VPC and i would be able to confiure
  the no.of subnets that i need to have and i would be able to confiure these according to the suites for our organization

2.What is Public Private Subnet and what are the differences between those two?
  By using a public subnet I would be able to host the resources which would be able to directly accessible from the public
  internet.
  By using Private subnet I would be able to host the instances which doesn't need to be accessed by the public internet directly
  and these would be typically the back end application server code or else the databases and the private subnet instances
  whould be able to downloads packages from the public internet directly by using NAT Gateway(route table) or SNAT Gateway(Load balancer)

3.How do you login to your private subnet instances?
  I would be using a bastion host in order to log to these private subnet instances and this bastion host would be residing
  inside the public subnet and we would be typically logging into the bastion host by using SSH and from the bastion host
  I would be logging into these private subnet instances by using the private IP

4.What will you do if you lose access to that private instance or lose the pem file?
  If i lost the access to EC2 instance right becasue of the pem file, I would be unmounting that particular voulme from that
  particular EC2 instance by stopping that instance and I would be attaching this particular voulme to anthor EC2 instance
  and I would be logging into that particular EC2 instance which i have the access to i will go to this particular directory
  and I would be modifying the authiorized keys and I would be modifying the .SSH authorized keys inside which i need to
  have the access too and i would be unmounting this particular voulme from the instance and i would be attaching it to the 
  older instance and i would be able to login by the new PEM file
-------------------------------------
Linux

5.Few LINUX commands used in your career?
  Multible commands which i would be using in day-to-day activities
  1. netstat -lntp  ->  in order to list the listening ports
  2. du -sh *  -> in order to check the usage
  3. lsblk  -> in order to list all the bloked devices
  4. ping  -> in order to test the connectivity
  5. telnet  -> in order to check the connectivity of a particular service in anther IP address
  6. htop top free -m  -> in order check the resouces of the EC2 instance
  7. ps -ef  -> in order to list all running  processers and i would be using | symbal in order to such particular process aswell
---------------------------------
Jenkins

6.have you worked on Jenkins, can you tell me what is Jenkins?
  Jenkins is a CICD tool by using which i would be able to manage the entire continuous Integration and contionus deployment
  from the creation of a Pull request to the deployment, continuously delivering our newly created images to the DEV or QA ENVTS.
  and we would be using multiple plugins in order to achieve this like: GIT plugin, Maven plugin, Sonor plugin in order to do
  sonor scans and we would be using kubenetes plugin

7.What kind of pipelines you have written in Jenkins?
  I have worked on multiple piples
  1. Declarative pipelines
  2. Scripted piplines
  3. Hybrid pipelines which is a mixture of 1,2 pipelines

8.Explain the declarative pipeline syntax used in Jenkins?
  The declarative pipeline following a particular syntax where these pipelines would be starting of with pipeline inside
  which i would be having a stages inside which i would be having stage and inside the stage i would be having the steps
  I would be having multiple stage in side the stages such that i would be able to configure these and i would be able to
  write the parallel as well in order to parallel execute multiple stages at a single time

9.Explain the Master Slave Architecture?
  By using a Master slave architecture, i would be able to reduce the load on master by configuring multiple slaves and
  running these Jenkins jobs on those slaves attached to it, in order to connect a linux agent as a slave to the Jenkins
  right, we need to go to the nodes add a new node I need to pass the IP of the salve which i need to attach as a slave
  i need to add the  pem file of the particular slave i need to pass the user directory and i need to also select the type
  of checks that Jenkins needs to be providing while connecting to this particular slave aswell and in order to if i want
  to connect a windows agent rt I need to download the JNP jar onto the windows Jenkins and I should be running the jnlp
  jar such that Jenkins would be connecting as slave to the Jenkins Master

10.Explain the CICD pipelines
  We would be having multiple stages to our CICD pipelines and these would be triggered via a webhooks which have configured
  So whenever a developer creates a pull request right this would be triggering the Jenkins pipeline via webhook where would
  be having multiple stages. 
  -> The first stage checkout stage where we would be checking out of the latest code and then 
  -> building the artifact in the second satge
  -> then we would be doing the sonor scans in order to figure out vulnerabilities code smells and code bugs in the 
     particular project
  -> in the fourth stage building the docker file
  -> in the fifth stage we would be using trivy scans in order to figure out any vulnerabilities inside a docker container as well
  -> in the sixth stage we would be pushing this particular image to the container registry and from there we would be
    upgrading our Dev or QA servers with latest image and then we would be running our QA automation test cases on top of
    the newly released build and sending the results as a post build actions to the developers aswell as QA team and we have
    used multiple plugins inside these pipelines in order to store the Jenkins credentials rt, we had used Jenkins credentials
    plugin and by using credentials ID inside these stages, we were able to check out those repositories as well and the same 
    way i have used credentials ID in order to authenticate with the docker and push all the images to the container repository
    as well

------------------------------
Ansible

11.What is Ansible and How it works?
  Ansible is a configuration management tool by using which i would be able to manage the configuration across multiple
  instances by using SSH communication.

12.What are different modules you have used in Ansible?
  I have used multiple module like: M module, Service Module, copy module, shell module these are the few module which i had
  used by using ansible roles, I would be able to organize my playbooks into reusable tasks and this is useful for the code management
  and usig these roles across multiple playbooks as well

13.What are handler and how are handlers used in ansible?
  handlers are ansible tasks which would be only triggred when triggred by a notified directive and these we would be using majorly
  in order to restart any services, even though if we trigger the handlers by using notified dierctor multiple times, these handlers
  would be executed only once at the end of the playbook execution
--------------------------------------
Terraform
14.What is Terraform and how it's used?
  Terraform is an infrastructure as a code tool by using which i would be able to create multiple resouces across multiple cloud
  providers by writing all the resources as a code

15.What is a provider and how it's used in Terrform?
  Providers are the plugins which would be taking to the respective backend API's, These would be in turn in creating those respective
  resources inside those respective provider.

16.How do you import the resources that have been manually created into terraform managment?
  For importing the resources which are created manually to terraform rt, We should be creating those respective terraform files
  and then we would be able to import by using terraform import resource, resource ID command

17.Can you explain some of the Terraform commands?
  -> terraform init - In order to initialize the terraform
  -> terraform plan - In order to plan what are the resources that it is going to create
  -> terraform apply - In order to create what ever the resources that we have planned
  -> terraform import - In order to import the resources
  -> terraform destroy - In order to destroy the resources that have been created or in the state file
  -> fmt - In order to format the terraform files
  -> terraform validate - in order to validate my terraform files

18.Can you Explain monolithic and microservices architecture?
  In monolithic architecture rt the entire application is entirely designed as a tightly coupled unit, and for suppose lets' say
  even if you want to make any modifications to a particular service rt, you should be deploying the entire application, and even 
  if you want to scale any of these services you should be deploying, you should be scalling the entire application as it's whole

  In the form of microservices rt we would be able to segregate this particular monolithic application into multiple individual
  microservices which are responsible for handling their own application code
  In the microservices rt we would be able to manage the scalling of the services efficiently why because we would be able to upscale
  or downscale a single service upon the usage of that particular service, and we will be using a Docker in order to containerize
  these mircroservices.
------------------------------------------
Docker

19.What is a Docker file?
  We would be having multiple instructions inside a docker file, FROM is the first line which should be defining the base image
  and the version of the particular docker file. and we would be having multiple build time commands like: RUN -> Which would be
  installing the packages inside the docker container. COPY -> which sould be copying the files from the local to the docker image.
  ADD -> which would be again copying the files or else download the files from the public repository as well as entering any tar
  balls as well, I would be using EXPOSE: which would be exposing the port, I would be also having commands like: CMD and ENTRY POINT
  which are runtime commands, which would be actually starting of a service.

20.Can you tell me the difference between COPY and ADD?
  COPY and ADD are Build time commands: where COPY whoud be able to just copy the files from local to the remote.
  and ADD: would be able to download the files and as well as the tarballs as well

21.Can you differantiate RUN and CMD?
  RUN is a built time command which would be installing packages in the docker container
  CMD is runtime command which would be actually starting of the service inside the docker container, when we execute the docker run

22.How can you diffentiate difference between CMD and entry point?
  CMD and Entry point both are docker run time commands, Which should be actually starting of the service.
  CMD: we would be able to override the parameters which we have passed and inside the entry point we won't be override the values
  even though if you mention multiple entry points rt only the last point will be executed.

23.Can you tell me some Docker commands which are used in Docker?
  -> Docker ps
  -> Docker ps -a
  -> docker logs
  -> docker xc 
  -> docker pull
  -> docker push
  -> docker run

24.When we have docker already why do we use Kubernetes?
  Docker is a containerization tool by using which i would be able to containerize all the dependencies and the artifacts
  Kubernetes is an orchestration tool by using which i would be able to better scale manage the docker containers which i have deployed

25.What are the different service are available in Kubernetes?
  there are multiple services in kubernetes:
  1. Load balancer - I would be able to expose my service to the outside the world
  2. Cluster IP - I would be able to create a Cluster IP which is accessible inside a cluster
  3. Node Port - I would be able to open a particular port to the kubernetes cluster

26.Node Efinity
  by using node efinity if we want to deploy some deployments in some of the worker nodes only.

27.Difference between replica set and stateful like where they are used?
  By using a replica set: i would be able to run on replica of docker container in all of the worker nodes which i have.
  By using the stateful set: I would be able to pursit the data across the docker life cycle or else container life cycle and by
  using replica sets, I would be able to manage the number of replicas that needs to be running for a particular deployment

28.How do you pass secrets to the deployments?
  In order to pass the secrets to the deployments, first of all i would be creating those secrets by using "kubectl create secret"
  if these secerts needs to be created from literals, I would be passing --literal and if these secrets needs to create from a file
  rt I would be creating --from a file and I would be mounting these as volumes inside my deployments.

29.can you tell me the difference between HPA and VPA?
  HPA: horizontal pod autoscaler, where i would be adding mutiple no.of replicas of the same deployment to the service such that it
  would be balacing the load and accessing it in the actual application as well.
  VPA: Virtical Pod Autoscaler, I would be adding  more resources to the same deployment which i was running if i am using VPA rt
  I need to have a downtime in order to add more resources to the same deployment which i was running.

30.How about Cluster Autoscaller in Kubernetes, can you please explain?
  Cluster Autoscaller when the HPA kicks and scheduling multiple pods rt in order to cater for the workload, the resources in the
  Kubernetes cluster would be exhausted so in order to create these resources on the fly or else the worker  nodes on fly we would
  be using a cluster autoscaller, so based on the resource usage in the worker nodes, we would be able to add more no.of worker 
  nodes by using a cluster autoscaller.
------------------------------------------------------------------------
Monitoring

31.What are the different tools you have used for monitoring?
  I have used "Promithiesus stack" in order to monitor the services and i have used "ELK stack" in order to monitor the logs and gather
  all the logs from the respective containers or else the respective services to a log aggregation such as elastic search and by
  using "kibana" we are creating multiple dashboards and sending the alerts based on the logs which we are getting by using watchers aswell

32.What are the different tools that are there in Promethus stack?
  InSide the promithus stack I would be having prometheus, grafana, alert manager and multiple exporters like JMX exporter and node
  exporter. I would be uusing JMX exporter in order to monitor the java services and i would be using NODE exporter in order to
  monitor the node metrices and all and we have multiple kinds of other exporters like: Elastic Seach exporter, MangoDB exporter,
  Kafka exporter in order to monitor the databases too. and these exporters would be running inside the host machines and exporting
  all of the metrices over particular port and by mentioning the prometheus do and by mentioning the scrape configs inside the 
  prometheus.yml, The prometheus would be able to scrape all of these matrices and would be able to store the data inside the
  prometheus time series database and 
  by using GRAFANA I would be able to create multiple dashboards, there are readily available dashboards which i would be able to
  directly copy by copying the ID of the particular dashboards or else I would be able to create these dashboards by using promql as well
  and based on the alert rules which i have configured i would be sending the alerts from the alert manager to different teams which
  i have configured inside my alert manager configuration file

33.How you monitor your kubernetes cluster?
  we have deployed Qube prometheus stack by using the readily avaliable helm chats and by using this qube prometheus stack, we were
  able to monitor certain kind of metrices that are responsible in oder to run the kubernetes cluster.
